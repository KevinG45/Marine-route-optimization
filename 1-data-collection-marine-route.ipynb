{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6799366,"sourceType":"datasetVersion","datasetId":3911893},{"sourceId":6799390,"sourceType":"datasetVersion","datasetId":3911912},{"sourceId":6799414,"sourceType":"datasetVersion","datasetId":3911927},{"sourceId":6799433,"sourceType":"datasetVersion","datasetId":3911941},{"sourceId":6799540,"sourceType":"datasetVersion","datasetId":3912006},{"sourceId":6799570,"sourceType":"datasetVersion","datasetId":3912027},{"sourceId":6799597,"sourceType":"datasetVersion","datasetId":3912048},{"sourceId":6799776,"sourceType":"datasetVersion","datasetId":3912165},{"sourceId":6799875,"sourceType":"datasetVersion","datasetId":3912239},{"sourceId":6813045,"sourceType":"datasetVersion","datasetId":3919261},{"sourceId":6813221,"sourceType":"datasetVersion","datasetId":3919319},{"sourceId":6813317,"sourceType":"datasetVersion","datasetId":3919346},{"sourceId":6874789,"sourceType":"datasetVersion","datasetId":3950453},{"sourceId":6874900,"sourceType":"datasetVersion","datasetId":3950509},{"sourceId":11214320,"sourceType":"datasetVersion","datasetId":7002692},{"sourceId":11214345,"sourceType":"datasetVersion","datasetId":7002712},{"sourceId":230608379,"sourceType":"kernelVersion"}],"dockerImageVersionId":30497,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kevinmathewsgeorge/1-data-collection-marine-route?scriptVersionId=244976701\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"#  **Marine Route Optimization Using Machine Learning**","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Collection and Preprocessing","metadata":{}},{"cell_type":"code","source":"# Imports \n\nimport requests # for making HTTP requests\n\nimport zipfile # fro working with zip files\n\nimport os # for interacting with OS\n\nimport pandas as pd\n\nimport ftplib # for FTP operations\n\nfrom getpass import getpass #to securely input a password\n\nimport xarray as xr # for working with multi-dimentional arrays and datasets\n\nimport numpy as np\n\nfrom datetime import datetime, timedelta \n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:04:47.177471Z","iopub.execute_input":"2025-03-31T00:04:47.177711Z","iopub.status.idle":"2025-03-31T00:04:47.379023Z","shell.execute_reply.started":"2025-03-31T00:04:47.177686Z","shell.execute_reply":"2025-03-31T00:04:47.378448Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"STEP 1: Collect data\n\nAIS and CMEMS data for the model on:\n\n* 01.01.2023\n* 01.04.2023\n* 01.07.2023\n* 01.10.2023\n\nCMEMS data for the routing on:\n\n* 01.06.2023\n* 02.06.2023\n* 03.06.2023\n\n# **Model data- AIS**","metadata":{}},{"cell_type":"code","source":"import requests\nimport zipfile\nimport os\nimport time\n\n'''urls = [\n    \"https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2023/AIS_2023_01_01.zip\",\n    \"https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2023/AIS_2023_04_01.zip\",\n    \"https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2020/AIS_2020_07_01.zip\",\n    \"https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2020/AIS_2020_10_01.zip\"\n]\n\ndef download_file(url, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            print(f\"Downloading {url} (Attempt {attempt+1}/{max_retries})...\")\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            filename = url.split('/')[-1]\n            with open(filename, \"wb\") as file:\n                file.write(response.content)\n            print(f\"Downloaded: {filename}\")\n            return filename  # Return the filename if successful\n        except requests.exceptions.RequestException as e:\n            print(f\"Error: {e}\")\n            time.sleep(5)  # Wait before retrying\n    print(f\"Failed to download {url} after {max_retries} attempts.\")\n    return None\n\ndef extract_zip(filename):\n    try:\n        with zipfile.ZipFile(filename, \"r\") as z:\n            z.extractall()\n        print(f\"Extracted: {filename}\")\n        os.remove(filename)\n    except zipfile.BadZipFile:\n        print(f\"Invalid ZIP file: {filename}\")\n\nfor url in urls:\n    file = download_file(url)\n    if file:\n        extract_zip(file)\n\nprint(\"Download completed!\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:04:47.379813Z","iopub.execute_input":"2025-03-31T00:04:47.379994Z","iopub.status.idle":"2025-03-31T00:04:47.386589Z","shell.execute_reply.started":"2025-03-31T00:04:47.379977Z","shell.execute_reply":"2025-03-31T00:04:47.385824Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, we define a list called 'urls' that contain 4 URLs pointing to ZIP files. These ZIP files contain data related to AIS (Automatic Identification System.)\n\nNext, we beging a loop to process each URL in the 'urls' list. \nInside the loop, it sends an **HTTP GET** request to the URL specified by url using the **requests.get method**. \nIt then extracts the last part of the URL (the filename) by splitting the URL using / and selecting the last element.\n\nNow, we are creating a new binary file with the same filename and writing the content of an HTTP response ('**r.content**') into that file. \n\nNext, we open the downloaded file as a ZIP archive using '**zipfile.ZipFile**'. If successful, it extrracts all the contents of the file to the current directoey using '**z.extractall()**'. If the extraction is succesful, it prints \"Extracted file\", and then the original ZIP file is removed using '**os.remove**'. If there's an issue with the opening the ZIP file, it prints \"Invalid file\"","metadata":{}},{"cell_type":"code","source":"# Read csv and create df\n\n'''ais_data = pd.DataFrame()\n\nfor url in urls:\n    \n    filename = url.split('/')[-1]\n    filename = filename.replace(\"zip\", \"csv\")\n    filepath = '/kaggle/working/' + filename  \n\n    data = pd.read_csv(filepath)\n    ais_data = pd.concat([ais_data, data], ignore_index=True)  # Concatenate DataFrames.\n\nprint(\"Before preprocessing...\")\nais_data\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:04:47.390387Z","iopub.execute_input":"2025-03-31T00:04:47.39057Z","iopub.status.idle":"2025-03-31T00:04:47.398407Z","shell.execute_reply.started":"2025-03-31T00:04:47.390554Z","shell.execute_reply":"2025-03-31T00:04:47.397741Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The goal here is to read CSV file and concatenate them into the 'ais_data' DataFrame.\n\nWe convert here the ZIP extension to CSV.\nThen we read the CSV file located at the 'filepath' and the data is loaded into a new pandas DataFrame 'data'.\n\nNext,the '**data**' DataFrame is concatenated with the '**ais_data**' DataFrame. The '**ignore_index=True**' argument ensures that the resulting DataFrame has a continuous index, as opposed to retaining the original indices from the individual DataFrames.","metadata":{}},{"cell_type":"code","source":"# Save AIS data to file\n\n#ais_data.to_csv('/kaggle/working/ais_data_all.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:04:47.399141Z","iopub.execute_input":"2025-03-31T00:04:47.399308Z","iopub.status.idle":"2025-03-31T00:04:47.41207Z","shell.execute_reply.started":"2025-03-31T00:04:47.399293Z","shell.execute_reply":"2025-03-31T00:04:47.41147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ais_data_all= pd.read_csv('/kaggle/input/ais-all-data-jan01-apr04-2023/ais_data_all.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:04:47.412697Z","iopub.execute_input":"2025-03-31T00:04:47.412876Z","iopub.status.idle":"2025-03-31T00:05:30.651978Z","shell.execute_reply.started":"2025-03-31T00:04:47.41286Z","shell.execute_reply":"2025-03-31T00:05:30.651379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ais_data_all","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:05:30.652936Z","iopub.execute_input":"2025-03-31T00:05:30.653573Z","iopub.status.idle":"2025-03-31T00:05:30.69043Z","shell.execute_reply.started":"2025-03-31T00:05:30.653548Z","shell.execute_reply":"2025-03-31T00:05:30.689712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model data - CMEMS","metadata":{}},{"cell_type":"markdown","source":"Downloading the data for the 2 days.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\"\"\"\nds1 = xr.open_dataset('/kaggle/input/jan2023-phy-ocean/cmems_mod_glo_phy_anfc_salinity0.083deg_PT1H-m_1698425838731.nc')\nds2 = xr.open_dataset('/kaggle/input/jan2023-phy-ocean/cmems_mod_glo_phy_anfc_theato0.083deg_PT1H-m_1698426299513.nc')\nds3 = xr.open_dataset('/kaggle/input/jan2023-phy-ocean/cmems_mod_glo_phy_anfc_thickness_0.083deg_P1D-m_1698437256627.nc')\n\n\n# Concatenation (if datasets have the same variables and dimensions)\nphy_jan = xr.merge([ds1, ds2, ds3])\n\nphy_jan.to_netcdf('jan_phy.nc')\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:05:30.691261Z","iopub.execute_input":"2025-03-31T00:05:30.691506Z","iopub.status.idle":"2025-03-31T00:05:30.697689Z","shell.execute_reply.started":"2025-03-31T00:05:30.691482Z","shell.execute_reply":"2025-03-31T00:05:30.697028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"phy_jan = xr.open_dataset('/kaggle/input/final-phy-jan-2023/jan_phy.nc')\nphy_jan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:05:30.698526Z","iopub.execute_input":"2025-03-31T00:05:30.698828Z","iopub.status.idle":"2025-03-31T00:05:31.220802Z","shell.execute_reply.started":"2025-03-31T00:05:30.698764Z","shell.execute_reply":"2025-03-31T00:05:31.220103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wav_jan = xr.open_dataset('/kaggle/input/global-wav-ocean-analysis-202301jan/cmems_mod_glo_wav_anfc_0.083deg_PT3H-i_1698305046266.nc')\nwav_jan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:05:31.221631Z","iopub.execute_input":"2025-03-31T00:05:31.221842Z","iopub.status.idle":"2025-03-31T00:05:31.302837Z","shell.execute_reply.started":"2025-03-31T00:05:31.221824Z","shell.execute_reply":"2025-03-31T00:05:31.302212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Downlaoding the data for 01 APRIL 2023\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Physics Data**","metadata":{}},{"cell_type":"code","source":"#April jana combination\n\"\"\"\nds1 = xr.open_dataset('/kaggle/input/apr-phy2023/cmems_mod_glo_phy_anfc_salanity_0.083deg_PT1H-m_1698436627345.nc')\nds2 = xr.open_dataset('/kaggle/input/apr-phy2023/cmems_mod_glo_phy_anfc_thetao_0.083deg_PT1H-m_1698437097509.nc')\nds3 = xr.open_dataset('/kaggle/input/apr-phy2023/cmems_mod_glo_phy_anfc_thickness_0.083deg_P1D-m_1698442161874.nc')\n\n\n# Concatenation (if datasets have the same variables and dimensions)\nphy_apr = xr.merge([ds1, ds2, ds3])\n\nphy_apr.to_netcdf('apr_phy.nc')\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:05:31.303741Z","iopub.execute_input":"2025-03-31T00:05:31.304013Z","iopub.status.idle":"2025-03-31T00:05:31.308764Z","shell.execute_reply.started":"2025-03-31T00:05:31.303989Z","shell.execute_reply":"2025-03-31T00:05:31.308094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"phy_apr = xr.open_dataset('/kaggle/input/final-phy-data-2023/apr_phy.nc')\nphy_apr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:05:31.309484Z","iopub.execute_input":"2025-03-31T00:05:31.309646Z","iopub.status.idle":"2025-03-31T00:05:31.436141Z","shell.execute_reply.started":"2025-03-31T00:05:31.309631Z","shell.execute_reply":"2025-03-31T00:05:31.43548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  **Wave Data**","metadata":{}},{"cell_type":"code","source":"wav_apr = xr.open_dataset('/kaggle/input/global-wav-ocean-analysis-202301apr/cmems_mod_glo_wav_anfc_0.083deg_PT3H-i_1698305442484.nc')\nwav_apr\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:05:31.437157Z","iopub.execute_input":"2025-03-31T00:05:31.437414Z","iopub.status.idle":"2025-03-31T00:05:31.522149Z","shell.execute_reply.started":"2025-03-31T00:05:31.43739Z","shell.execute_reply":"2025-03-31T00:05:31.521452Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  **Selecting ship route area**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\n# Define the bounding box coordinates\nbbox = ((-65, 20), (-100, 43))\n\n# Create a Matplotlib figure and axis with a specific Cartopy projection\nfig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n\n# Add map features such as coastlines and borders\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS)\n\n# Set the map extent based on the bounding box\nax.set_extent([bbox[0][0], bbox[1][0], bbox[0][1], bbox[1][1]])\n\n# Add text labels for country names and port names\nlabel_properties = {\n    'fontweight': 'normal',\n    'fontsize': 10,\n    'ha': 'center',\n    'va': 'center',\n}\n\n# Define city and country labels with coordinates\nlabels = {\n    'Washington': (-73.69, 40.82, 'USA', 'red'),\n    \n    'Havana': (-82.3, 23.1, 'Mexico', 'red'),\n}\n\nfor city, (lon, lat, country, color) in labels.items():\n    ax.text(lon, lat, city, color=color, **label_properties)\n    ax.text(lon, lat - 1, country, color=color, **label_properties)\n\n# Differentiate between land and ocean by setting colors\nland = cfeature.NaturalEarthFeature('physical', 'land', '50m', edgecolor='face', facecolor='darkgrey')\nocean = cfeature.NaturalEarthFeature('physical', 'ocean', '50m', edgecolor='face', facecolor='lightblue')\n\nax.add_feature(land, zorder=0)\nax.add_feature(ocean, zorder=0)\n\n# Add a title\nplt.title(\"Shipping route area between port Dubai and Mumbai\")\n\n# Show the map\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:05:31.523073Z","iopub.execute_input":"2025-03-31T00:05:31.523322Z","iopub.status.idle":"2025-03-31T00:06:05.508623Z","shell.execute_reply.started":"2025-03-31T00:05:31.523298Z","shell.execute_reply":"2025-03-31T00:06:05.507882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Reduce CMEMS wave data size by using bbox**","metadata":{}},{"cell_type":"code","source":"\n#bbox = ((-60, 20), (-120, 50))\n\n\ndef get_closest(array, value):\n    return np.abs(array - value).argmin()\n    \nds_wav_jan = wav_jan\nds_wav_apr = wav_apr\n\n\nds_wav_all = [ds_wav_jan, ds_wav_apr]\n\ndatasets_wav = []\n\nfor ds_month in ds_wav_all:\n  \n  lon_min = get_closest(ds_month.longitude.data, bbox[0][0])\n  lon_max = get_closest(ds_month.longitude.data, bbox[1][0])\n  lat_min = get_closest(ds_month.latitude.data, bbox[0][1])\n  lat_max = get_closest(ds_month.latitude.data, bbox[1][1])\n\n  ds_wav_month_reg = ds_month.isel(time = 0, longitude = slice(lon_min, lon_max), latitude = slice(lat_min, lat_max))\n  datasets_wav.append(ds_wav_month_reg)\n\nds_wav = xr.concat(datasets_wav, dim = 'time')\nds_wav\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:05.509515Z","iopub.execute_input":"2025-03-31T00:06:05.509725Z","iopub.status.idle":"2025-03-31T00:06:05.535483Z","shell.execute_reply.started":"2025-03-31T00:06:05.509707Z","shell.execute_reply":"2025-03-31T00:06:05.534809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Reduce CMEMS physics data size by using bbox**","metadata":{}},{"cell_type":"code","source":"#bbox = ((-60, 20), (-120, 50))\n\n\nds_phy_jan =  phy_jan\nds_phy_apr =  phy_apr\n\n\nds_phy_all = [ds_phy_jan, ds_phy_apr]\n\ndatasets_phy = []\n\nfor ds_month in ds_phy_all:\n  \n  lon_min = get_closest(ds_month.longitude.data, bbox[0][0])\n  lon_max = get_closest(ds_month.longitude.data, bbox[1][0])\n  lat_min = get_closest(ds_month.latitude.data, bbox[0][1])\n  lat_max = get_closest(ds_month.latitude.data, bbox[1][1])\n\n  ds_phy_month_reg = ds_month.isel(time = 0, longitude = slice(lon_min, lon_max), latitude = slice(lat_min, lat_max))\n  datasets_phy.append(ds_phy_month_reg)\n\nds_phy = xr.concat(datasets_phy, dim = 'time')\nds_phy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:05.536486Z","iopub.execute_input":"2025-03-31T00:06:05.536684Z","iopub.status.idle":"2025-03-31T00:06:05.558731Z","shell.execute_reply.started":"2025-03-31T00:06:05.536667Z","shell.execute_reply":"2025-03-31T00:06:05.558035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Join all wave products by using open_mfdataset, chunking data in response to memory issues\nds_wav_all = xr.open_mfdataset('/kaggle/input/wave-data/*.nc')\n\nds_wav_all","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:05.559461Z","iopub.execute_input":"2025-03-31T00:06:05.559648Z","iopub.status.idle":"2025-03-31T00:06:05.781849Z","shell.execute_reply.started":"2025-03-31T00:06:05.559632Z","shell.execute_reply":"2025-03-31T00:06:05.781164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds_phy_all = xr.open_mfdataset('/kaggle/input/phy-data/*.nc')\nds_phy_all","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:05.782702Z","iopub.execute_input":"2025-03-31T00:06:05.782933Z","iopub.status.idle":"2025-03-31T00:06:05.963883Z","shell.execute_reply.started":"2025-03-31T00:06:05.782909Z","shell.execute_reply":"2025-03-31T00:06:05.963268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Routing Data -  CMEMS**","metadata":{}},{"cell_type":"code","source":"\nphy_jun = xr.open_dataset('/kaggle/input/routing-ocean-physics-202301-june/cmems_mod_glo_phy-cur_anfc_0.083deg_PT6H-i_1698267716666.nc')\nphy_jun","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:05.964787Z","iopub.execute_input":"2025-03-31T00:06:05.965065Z","iopub.status.idle":"2025-03-31T00:06:06.014114Z","shell.execute_reply.started":"2025-03-31T00:06:05.965022Z","shell.execute_reply":"2025-03-31T00:06:06.013476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nwav_jun = xr.open_dataset('/kaggle/input/routing-ocean-wav-202301-june/cmems_mod_glo_wav_anfc_0.083deg_PT3H-i_1698308781382.nc')\nwav_jun","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:06.01489Z","iopub.execute_input":"2025-03-31T00:06:06.015165Z","iopub.status.idle":"2025-03-31T00:06:06.082237Z","shell.execute_reply.started":"2025-03-31T00:06:06.015143Z","shell.execute_reply":"2025-03-31T00:06:06.081557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **STEP 2: Merge and Preprocess Model Data**","metadata":{}},{"cell_type":"markdown","source":"\nIn general:\n\n* Merge AIS data with CMEMS data\n* Remove unneccessary columns (show estimated time, latitude, longitude, heading, SOG, COG, Gross Tonage, VHM0, VMDR, VTPK, Temperature, Salinity and Thickness)\n\nFor all data:\n\n* Remove ships with Status =! 0 or Status =! 8\n* Remove ships with SOG < 7 or SOG > 102.2\n* Remove ships with latitude > 91 and longitude > 181\n* Remove ships with latitude < -91 and longitude < -181\n* Remove ships with heading > 361\n* Round data base times\n* Normalize all data to remove outliers\n* Standardize between 0 and 1","metadata":{}},{"cell_type":"code","source":"study_data = pd.read_csv('/kaggle/input/ais-all-data-jan01-apr04-2023/ais_data_all.csv')\nstudy_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:06.083037Z","iopub.execute_input":"2025-03-31T00:06:06.083272Z","iopub.status.idle":"2025-03-31T00:06:29.997012Z","shell.execute_reply.started":"2025-03-31T00:06:06.083254Z","shell.execute_reply":"2025-03-31T00:06:29.99629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Scatter plot of 'LAT' and 'LON'\nplt.scatter(study_data['LON'], study_data['LAT'], s=1)  # Adjust 's' for point size\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Spatial Distribution of AIS Data')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:29.997876Z","iopub.execute_input":"2025-03-31T00:06:29.998105Z","iopub.status.idle":"2025-03-31T00:06:33.338381Z","shell.execute_reply.started":"2025-03-31T00:06:29.998087Z","shell.execute_reply":"2025-03-31T00:06:33.337525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove ships with status =! 0 and status =! 8\nstudy_data = study_data[(study_data['Status'] == 0) | (study_data['Status'] == 8)].dropna()\n\n# Remove ships with SOG < 5 or SOG > 102.2\nstudy_data = study_data[(study_data['SOG'] > 7) & (study_data['SOG'] < 102.2)].dropna()\n\n# Remove ships with latitude > 91 and longitude > 181\nstudy_data = study_data[(study_data['LAT'] >= 20) & (study_data['LAT'] <= 43)].dropna()\nstudy_data = study_data[(study_data['LON'] >= -87.5) & (study_data['LON'] <= -72.5)].dropna()\n\n# Remove ships with heading > 361\nstudy_data = study_data[(study_data['Heading'] < 361)].dropna()\n\n# Calculate tonnage (Length * Breadth * Depth * S) - WE DON'T HAVE THE DEPTH\n# According to https://cdn.shopify.com/s/files/1/1021/8837/files/Tonnage_Guide_1_-_Simplified_Measurement.pdf?1513\nstudy_data['GrossTonnage'] = 0.67 * study_data['Length'] * study_data['Width']\n\nstudy_data = study_data.reset_index(drop=True)\nstudy_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:33.339428Z","iopub.execute_input":"2025-03-31T00:06:33.339706Z","iopub.status.idle":"2025-03-31T00:06:39.322929Z","shell.execute_reply.started":"2025-03-31T00:06:33.339682Z","shell.execute_reply":"2025-03-31T00:06:39.322183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Scatter plot of 'LAT' and 'LON'\nplt.scatter(study_data['LON'], study_data['LAT'], s=1)  # Adjust 's' for point size\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Spatial Distribution of AIS Data')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:39.324139Z","iopub.execute_input":"2025-03-31T00:06:39.324429Z","iopub.status.idle":"2025-03-31T00:06:39.566879Z","shell.execute_reply.started":"2025-03-31T00:06:39.324404Z","shell.execute_reply":"2025-03-31T00:06:39.566244Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we round the data base times.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime, timedelta\n\ndef datetime_rounder(time_str):\n    time = datetime.fromisoformat(time_str)\n    rounded_time = time.replace(second=0, minute=0, hour=time.hour) + timedelta(minutes=time.minute // 30)\n    return rounded_time.strftime('%y-%m-%d %H:%M:%S')\n\nstudy_data['EstimatedTime'] = study_data['BaseDateTime'].apply(datetime_rounder)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:39.567912Z","iopub.execute_input":"2025-03-31T00:06:39.568198Z","iopub.status.idle":"2025-03-31T00:06:40.885164Z","shell.execute_reply.started":"2025-03-31T00:06:39.568174Z","shell.execute_reply":"2025-03-31T00:06:40.884328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"study_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:40.886577Z","iopub.execute_input":"2025-03-31T00:06:40.886888Z","iopub.status.idle":"2025-03-31T00:06:40.910289Z","shell.execute_reply.started":"2025-03-31T00:06:40.886853Z","shell.execute_reply":"2025-03-31T00:06:40.909451Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Standardization between 0 and 1","metadata":{}},{"cell_type":"code","source":"\n# Get data for one time\nds_wav = ds_wav_all\nds_phy = ds_phy_all\nlat = round(study_data['LAT'].iloc[0])\nlon = round(study_data['LON'].iloc[0])\ntime = str(study_data['EstimatedTime'].iloc[0])\n\nVHM0 = ds_wav.VHM0.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\nVMDR = ds_wav.VMDR.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n#thetao = ds_phy.thetao.sel(time = time, longitude = lon, latitude = lat, depth = 0, method = 'nearest')\n#salin = ds_phy.so.sel(time = time, longitude = lon, latitude = lat, depth = 0, method = 'nearest')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:40.914888Z","iopub.execute_input":"2025-03-31T00:06:40.915216Z","iopub.status.idle":"2025-03-31T00:06:40.929819Z","shell.execute_reply.started":"2025-03-31T00:06:40.915193Z","shell.execute_reply":"2025-03-31T00:06:40.929112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"study_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:06:40.93062Z","iopub.execute_input":"2025-03-31T00:06:40.930846Z","iopub.status.idle":"2025-03-31T00:06:40.955325Z","shell.execute_reply.started":"2025-03-31T00:06:40.930828Z","shell.execute_reply":"2025-03-31T00:06:40.954581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# utilizing 6 cores below ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:12:24.565882Z","iopub.execute_input":"2025-03-31T00:12:24.566201Z","iopub.status.idle":"2025-03-31T00:12:24.570157Z","shell.execute_reply.started":"2025-03-31T00:12:24.566181Z","shell.execute_reply":"2025-03-31T00:12:24.56893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Create new columns with initial values\nstudy_data['VHM0'] = 0.0\nstudy_data['VMDR'] = 0.0\nstudy_data['VTPK'] = 0.0\nstudy_data['Temperature'] = 0.0\nstudy_data['Salinity'] = 0.0\nstudy_data['Thickness'] = 0.0\n\ndef extract_model(args):\n    index, row = args\n    lat = round(row['LAT'], 1)\n    lon = round(row['LON'], 1)\n    time = str(row['EstimatedTime'])\n\n    VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n    VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n    VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n    #thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n    #salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n    #thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n    study_data.at[index, 'VHM0'] = VHM0\n    study_data.at[index, 'VMDR'] = VMDR\n    study_data.at[index, 'VTPK'] = VTPK\n    #study_data.at[index, 'Temperature'] = thetao\n    #study_data.at[index, 'Salinity'] = salin\n    #study_data.at[index, 'Thickness'] = thickness\n\n# Use ThreadPoolExecutor to parallelize the processing\nwith ThreadPoolExecutor(max_workers=6) as executor:\n    list(tqdm(executor.map(extract_model, study_data.iterrows()), total=len(study_data), desc=\"Processing\"))\n\n# Drop rows with missing values\nstudy_data = study_data.dropna()\n\n\n# Save merge data (not fully processed)\nstudy_data.to_csv('/kaggle/working/study_data_np.csv')\n\n# Display the processed data\nstudy_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:12:29.594471Z","iopub.execute_input":"2025-03-31T00:12:29.59476Z","iopub.status.idle":"2025-03-31T01:18:09.84142Z","shell.execute_reply.started":"2025-03-31T00:12:29.59474Z","shell.execute_reply":"2025-03-31T01:18:09.840513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The goal is to merge AIS (Automatic Identification System) and CMEMS (Copernicus Marine Environment Monitoring Service) data. The code uses a dataset named study_data and performs the following steps:\n\n**Data Preprocessing:**\nRows with missing values are dropped from the study_data dataset using study_data = study_data.dropna().\n\n**Column Initialization:**\nNew columns (VHM0, VMDR, VTPK, Temperature, Salinity, Thickness) are added to the study_data DataFrame, each initialized with the value 0.0.\n\nData Extraction:\nThe extract_model function is defined to extract specific data from other datasets (ds_wav and ds_phy) based on the values in each row of the study_data DataFrame.\nFor each row in study_data:\nThe latitude (lat), longitude (lon), and time (time) values are rounded and converted to strings.\nData is extracted from the ds_wav (wave data) and ds_phy (physical data) datasets using the rounded latitude, longitude, and time values.\nExtracted values are assigned to the corresponding columns in the study_data DataFrame.\nProcessing Progress Bar:\n\nThe tqdm library is used to create a progress bar that shows the processing progress for the iteration through rows in study_data.\nFunction Invocation and Data Display:\n\nThe extract_model function is called to perform the data extraction and assignment for each row.\nRows with missing values are dropped again from the study_data DataFrame.\nThe processed data is displayed.","metadata":{}},{"cell_type":"code","source":"'''from tqdm import tqdm\nimport pandas as pd\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Create new columns with initial values\nstudy_data['VHM0'] = 0.0\nstudy_data['VMDR'] = 0.0\nstudy_data['VTPK'] = 0.0\n#study_data['Temperature'] = 0.0\n#study_data['Salinity'] = 0.0\n#study_data['Thickness'] = 0.0\n\ndef extract_model():\n    # Use tqdm to create a progress bar\n    for index, row in tqdm(study_data.iterrows(), total=len(study_data), desc=\"Processing\"):\n        lat = round(row['LAT'], 1)\n        lon = round(row['LON'], 1)\n        time = str(row['EstimatedTime'])\n\n        VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        #thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        #salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        #thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n        study_data.at[index, 'VHM0'] = VHM0\n        study_data.at[index, 'VMDR'] = VMDR\n        study_data.at[index, 'VTPK'] = VTPK\n        #study_data.at[index, 'Temperature'] = thetao\n        #study_data.at[index, 'Salinity'] = salin\n        #study_data.at[index, 'Thickness'] = thickness\n\n# Call the function to extract the model\nextract_model()\n\n# Drop rows with missing values\nstudy_data = study_data.dropna()\n\n# Save merge data (not fully processed)\nstudy_data.to_csv('/kaggle/working/study_data_np.csv')\n\n# Display the processed data\nstudy_data\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:12:12.38424Z","iopub.status.idle":"2025-03-31T00:12:12.384597Z","shell.execute_reply.started":"2025-03-31T00:12:12.384416Z","shell.execute_reply":"2025-03-31T00:12:12.38444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"from tqdm import tqdm\nimport pandas as pd\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Create new columns with initial values\nstudy_data['VHM0'] = 0.0\nstudy_data['VMDR'] = 0.0\nstudy_data['VTPK'] = 0.0\nstudy_data['Temperature'] = 0.0\nstudy_data['Salinity'] = 0.0\nstudy_data['Thickness'] = 0.0\n\ndef extract_model(chunk):\n    # Use tqdm to create a progress bar\n    for index, row in tqdm(chunk.iterrows(), total=len(chunk), desc=\"Processing\"):\n        lat = round(row['LAT'], 1)\n        lon = round(row['LON'], 1)\n        time = str(row['EstimatedTime'])\n\n        VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n        study_data.at[index, 'VHM0'] = VHM0\n        study_data.at[index, 'VMDR'] = VMDR\n        study_data.at[index, 'VTPK'] = VTPK\n        study_data.at[index, 'Temperature'] = thetao\n        study_data.at[index, 'Salinity'] = salin\n        study_data.at[index, 'Thickness'] = thickness\n\n# Define the chunk size (adjust as needed)\nchunk_size = 100000\n\n# Split the data into chunks\nchunks = [study_data[i:i + chunk_size] for i in range(0, len(study_data), chunk_size)]\n\n# Process each chunk and store the results\nfor chunk in chunks:\n    extract_model(chunk)\n\n# Drop rows with missing values\nstudy_data = study_data.dropna()\n\n# Display the processed data\nstudy_data\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:12:12.3864Z","iopub.status.idle":"2025-03-31T00:12:12.386753Z","shell.execute_reply.started":"2025-03-31T00:12:12.386586Z","shell.execute_reply":"2025-03-31T00:12:12.386603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"#to check processing progress\nfrom tqdm import tqdm\n\n\n# Merge AIS and CMEMS datafrom tqdm import tqdm\n\n# Define the chunk size (adjust as needed)\nchunk_size = 100000\n\n# Merge AIS and CMEMS data\nstudy_data = study_data.dropna()\n\n# Initialize empty lists to store results\nvhm0_values = []\nvmdr_values = []\nvtpk_values = []\ntemperature_values = []\nsalinity_values = []\nthickness_values = []\n\ndef extract_model(chunk):\n    vhm0_chunk = []\n    vmdr_chunk = []\n    vtpk_chunk = []\n    temperature_chunk = []\n    salinity_chunk = []\n    thickness_chunk = []\n\n    for index, row in tqdm(chunk.iterrows(), total=len(chunk), desc=\"Processing\"):\n        lat = round(row['LAT'], 1)\n        lon = round(row['LON'], 1)\n        time = str(row['EstimatedTime'])\n\n        VHM0 = ds_wav.VHM0.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VMDR = ds_wav.VMDR.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        VTPK = ds_wav.VTPK.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n        thetao = ds_phy.thetao.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        salin = ds_phy.so.sel(time=time, longitude=lon, latitude=lat, depth=0, method='nearest')\n        thickness = ds_phy.mlotst.sel(time=time, longitude=lon, latitude=lat, method='nearest')\n\n        vhm0_chunk.append(VHM0)\n        vmdr_chunk.append(VMDR)\n        vtpk_chunk.append(VTPK)\n        temperature_chunk.append(thetao)\n        salinity_chunk.append(salin)\n        thickness_chunk.append(thickness)\n\n    return vhm0_chunk, vmdr_chunk, vtpk_chunk, temperature_chunk, salinity_chunk, thickness_chunk\n\n# Split the data into smaller chunks\nchunks = [study_data[i:i + chunk_size] for i in range(0, len(study_data), chunk_size)]\n\n# Process each chunk and store the results\nfor chunk in chunks:\n    vhm0_chunk, vmdr_chunk, vtpk_chunk, temperature_chunk, salinity_chunk, thickness_chunk = extract_model(chunk)\n    vhm0_values.extend(vhm0_chunk)\n    vmdr_values.extend(vmdr_chunk)\n    vtpk_values.extend(vtpk_chunk)\n    temperature_values.extend(temperature_chunk)\n    salinity_values.extend(salinity_chunk)\n    thickness_values.extend(thickness_chunk)\n\n# Assign the results to the DataFrame\nstudy_data['VHM0'] = vhm0_values\nstudy_data['VMDR'] = vmdr_values\nstudy_data['VTPK'] = vtpk_values\nstudy_data['Temperature'] = temperature_values\nstudy_data['Salinity'] = salinity_values\nstudy_data['Thickness'] = thickness_values\n\n# Now, study_data contains the processed data\nstudy_data = study_data.dropna()\n\nstudy_data['VHM0'] = 0.0\nstudy_data['VMDR'] = 0.0\nstudy_data['VTPK'] = 0.0\nstudy_data['Temperature'] = 0.0\nstudy_data['Salinity'] = 0.0\nstudy_data['Thickness'] = 0.0\n\ndef extract_model():\n  \n  for index, row in study_data.iterrows():\n\n      lat = round(row['LAT'], 1)\n      lon = round(row['LON'], 1)\n      time = str(row['EstimatedTime'])\n\n      VHM0 = ds_wav.VHM0.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n      VMDR = ds_wav.VMDR.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n      VTPK = ds_wav.VTPK.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n      thetao = ds_phy.thetao.sel(time = time, longitude = lon, latitude = lat, depth = 0, method = 'nearest')\n      salin = ds_phy.so.sel(time = time, longitude = lon, latitude = lat, depth = 0, method = 'nearest')\n      thickness = ds_phy.mlotst.sel(time = time, longitude = lon, latitude = lat, method = 'nearest')\n\n      study_data.at[index, 'VHM0'] = VHM0\n      study_data.at[index, 'VMDR'] = VMDR\n      study_data.at[index, 'VTPK'] = VTPK\n      study_data.at[index, 'Temperature'] = thetao\n      study_data.at[index, 'Salinity'] = salin\n      study_data.at[index, 'Thickness'] = thickness\n      \nextract_model()\n\nstudy_data = study_data.dropna()\n\n# Save merge data (not fully processed)\nstudy_data.to_csv('data/study_data_np.csv')\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T00:12:12.388087Z","iopub.status.idle":"2025-03-31T00:12:12.388417Z","shell.execute_reply.started":"2025-03-31T00:12:12.388249Z","shell.execute_reply":"2025-03-31T00:12:12.388265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* **study_data = study_data.dropna()**: This line removes rows with missing values (NaN) from the study_data DataFrame.\n\n* The next set of lines initializes columns in the study_data DataFrame with zeros. These columns are intended to store various data points from the CMEMS datasets: VHM0, VMDR, VTPK, Temperature, Salinity, and Thickness.\n\n* **def extract_model()**:: This line defines a function called extract_model.\n\n* for index, row in study_data.iterrows():: This line starts a loop that iterates through each row in the study_data DataFrame. For each row, the code will extract data from the CMEMS datasets based on the latitude, longitude, and estimated time for that row.\n\n* **lat = round(row['LAT'], 1)**: This line extracts the latitude value from the current row and rounds it to one decimal place.\n\n* **lon = round(row['LON'], 1)**: This line extracts the longitude value from the current row and rounds it to one decimal place.\n\n* **time = str(row['EstimatedTime'])**: This line extracts the estimated time value from the current row and converts it to a string.\n\n* The following lines use the **.sel()** method to extract data from the CMEMS datasets (e.g., ds_wav.VHM0.sel(...), ds_phy.thetao.sel(...), etc.) based on the latitude, longitude, and time values obtained from the current row.\n\n* The extracted data is then assigned to the corresponding columns in the study_data DataFrame (e.g., study_data.at[index, 'VHM0'] = VHM0). This essentially populates the DataFrame with the CMEMS data for each row in the study_data DataFrame.\n\n* Finally, after processing all rows, the code calls **extract_model()** to perform the data extraction. Then, it removes any rows in study_data that still have missing values (NaN) using **study_data = study_data.dropna()**.\n\nThe purpose of this code is to enrich the study_data DataFrame with CMEMS data based on the provided latitude, longitude, and estimated time. It's essentially merging the AIS data with relevant CMEMS data for further analysis.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize COG\nstudy_data['COG_norm'] = (study_data['COG'] - study_data['COG'].mean(axis = 0)) / study_data['COG'].std(axis = 0)\n\n# Make histogram of normalized COG\nstudy_data['COG_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized COG', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized COG', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['COG_norm'] = ((study_data['COG_norm'] - study_data['COG_norm'].min(axis=0)) / \n                           (study_data['COG_norm'].max(axis = 0) - study_data['COG_norm'].min(axis=0)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:23:02.91295Z","iopub.execute_input":"2025-03-31T01:23:02.913306Z","iopub.status.idle":"2025-03-31T01:23:03.19162Z","shell.execute_reply.started":"2025-03-31T01:23:02.913283Z","shell.execute_reply":"2025-03-31T01:23:03.190778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize Gross Tonnage\nstudy_data['GrossTonnage_norm'] = (study_data['GrossTonnage'] - study_data['GrossTonnage'].mean(axis = 0)) / study_data['GrossTonnage'].std(axis = 0)\n\n# Make histogram of normalized Gross Tonnage\nstudy_data['GrossTonnage_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized Gross Tonnage', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized Gross Tonnage', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['GrossTonnage_norm'] = ((study_data['GrossTonnage_norm'] - study_data['GrossTonnage_norm'].min(axis=0)) / \n                                    (study_data['GrossTonnage_norm'].max(axis = 0) - study_data['GrossTonnage_norm'].min(axis=0)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:23:06.907213Z","iopub.execute_input":"2025-03-31T01:23:06.907503Z","iopub.status.idle":"2025-03-31T01:23:07.136338Z","shell.execute_reply.started":"2025-03-31T01:23:06.907481Z","shell.execute_reply":"2025-03-31T01:23:07.135557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize VHM0\nstudy_data['VHM0_norm'] = (study_data['VHM0'] - study_data['VHM0'].mean(axis = 0)) / study_data['VHM0'].std(axis = 0)\n\n# Make histogram of normalized VHM0\nstudy_data['VHM0_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized VHM0', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized VHM0', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Remove VHM0 outliers\nstudy_data = study_data[(study_data['VHM0_norm'] > -2)].dropna()\n\n# Standardize to 0 to 1\nstudy_data['VHM0_norm'] = ((study_data['VHM0_norm'] - study_data['VHM0_norm'].min(axis=0)) / \n                            (study_data['VHM0_norm'].max(axis = 0) - study_data['VHM0_norm'].min(axis=0)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:23:10.800992Z","iopub.execute_input":"2025-03-31T01:23:10.801296Z","iopub.status.idle":"2025-03-31T01:23:11.384011Z","shell.execute_reply.started":"2025-03-31T01:23:10.801277Z","shell.execute_reply":"2025-03-31T01:23:11.383225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize VMDR\nstudy_data['VMDR_norm'] = (study_data['VMDR'] - study_data['VMDR'].mean(axis = 0)) / study_data['VMDR'].std(axis = 0)\n\n# Make histogram of normalized VMDR\nstudy_data['VMDR_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized VMDR', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized VMDR', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['VMDR_norm'] = ((study_data['VMDR_norm'] - study_data['VMDR_norm'].min(axis=0)) / \n                            (study_data['VMDR_norm'].max(axis = 0) - study_data['VMDR_norm'].min(axis=0)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:23:14.355379Z","iopub.execute_input":"2025-03-31T01:23:14.355644Z","iopub.status.idle":"2025-03-31T01:23:14.57336Z","shell.execute_reply.started":"2025-03-31T01:23:14.355626Z","shell.execute_reply":"2025-03-31T01:23:14.572623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize VTPK\nstudy_data['VTPK_norm'] = (study_data['VTPK'] - study_data['VTPK'].mean(axis = 0)) / study_data['VTPK'].std(axis = 0)\n\n# Make histogram of normalized VTPK\nstudy_data['VTPK_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized VTPK', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized VTPK', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['VTPK_norm'] = ((study_data['VTPK_norm'] - study_data['VTPK_norm'].min(axis=0)) / \n                            (study_data['VTPK_norm'].max(axis=0) - study_data['VTPK_norm'].min(axis=0)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:23:20.055072Z","iopub.execute_input":"2025-03-31T01:23:20.055378Z","iopub.status.idle":"2025-03-31T01:23:20.29375Z","shell.execute_reply.started":"2025-03-31T01:23:20.055357Z","shell.execute_reply":"2025-03-31T01:23:20.292994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize Temperature\nstudy_data['Temperature_norm'] = (study_data['Temperature'] - study_data['Temperature'].mean(axis = 0)) / study_data['Temperature'].std(axis = 0)\n\n# Make histogram of normalized Temperature\nstudy_data['Temperature_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized Temperature', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized Temperature', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['Temperature_norm'] = ((study_data['Temperature_norm'] - study_data['Temperature_norm'].min(axis=0)) / \n                                   (study_data['Temperature_norm'].max(axis = 0) - study_data['Temperature_norm'].min(axis=0)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:23:23.747675Z","iopub.execute_input":"2025-03-31T01:23:23.747949Z","iopub.status.idle":"2025-03-31T01:23:23.917799Z","shell.execute_reply.started":"2025-03-31T01:23:23.74793Z","shell.execute_reply":"2025-03-31T01:23:23.917113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# Normalize Salinity\nstudy_data['Salinity_norm'] = (study_data['Salinity'] - study_data['Salinity'].mean(axis = 0)) / study_data['Salinity'].std(axis = 0)\n\n# Make histogram of normalized Salinity\nstudy_data['Salinity_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized Salinity', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized Salinity', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['Salinity_norm'] = ((study_data['Salinity_norm'] - study_data['Salinity_norm'].min(axis=0)) / \n                               (study_data['Salinity_norm'].max(axis=0) - study_data['Salinity_norm'].min(axis=0)))'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:23:27.49675Z","iopub.execute_input":"2025-03-31T01:23:27.497099Z","iopub.status.idle":"2025-03-31T01:23:27.700008Z","shell.execute_reply.started":"2025-03-31T01:23:27.497072Z","shell.execute_reply":"2025-03-31T01:23:27.698873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# Normalize Thickness\nstudy_data['Thickness_norm'] = (study_data['Thickness'] - study_data['Thickness'].mean(axis = 0)) / study_data['Thickness'].std(axis = 0)\n\n# Make histogram of normalized Thickness\nstudy_data['Thickness_norm'].plot(kind = 'hist', alpha = 0.7, bins = 30, title = 'Histogram of normalized Thickness', \n                            grid = True, figsize = (10,5), fontsize = 14, color = ['#C40000'])\nplt.xlabel('Normalized Thickness', fontsize = 16)\nplt.ylabel(\"Frequency\", fontsize = 16)\nplt.show()\n\n# Standardize to 0 to 1\nstudy_data['Thickness_norm'] = ((study_data['Thickness_norm'] - study_data['Thickness_norm'].min(axis=0)) / \n                               (study_data['Thickness_norm'].max(axis=0) - study_data['Thickness_norm'].min(axis=0)))'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:23:31.721685Z","iopub.execute_input":"2025-03-31T01:23:31.722474Z","iopub.status.idle":"2025-03-31T01:23:31.919123Z","shell.execute_reply.started":"2025-03-31T01:23:31.722454Z","shell.execute_reply":"2025-03-31T01:23:31.918378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"After preprocessing...\")\nfinal_data = study_data.filter(['EstimatedTime', 'LAT', 'LON', 'Heading', 'SOG_norm', 'COG_norm', 'GrossTonnage_norm', 'VHM0_norm', 'VMDR_norm', 'VTPK_norm'])\nfinal_data = final_data.dropna()\nfinal_data.to_csv('/kaggle/working/preprocessed_data.csv')\nfinal_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:24:51.8041Z","iopub.execute_input":"2025-03-31T01:24:51.804987Z","iopub.status.idle":"2025-03-31T01:24:54.433787Z","shell.execute_reply.started":"2025-03-31T01:24:51.804963Z","shell.execute_reply":"2025-03-31T01:24:54.432933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}